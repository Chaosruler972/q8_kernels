{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3611.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/usr/lib/python3/dist-packages/setuptools/version.py:1: UserWarning: Module q8_matmul was already imported from None, but /root/linear_layers is being added to sys.path\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from typing import *\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from q8_matmul.ops._C import rope\n",
    "\n",
    "frame_rate = 25\n",
    "\n",
    "video_scale_factor = 8\n",
    "vae_scale_factor = 32\n",
    "\n",
    "height = 480\n",
    "width = 720\n",
    "num_frames = 81 \n",
    "num_frames = ((num_frames - 2) // 8 + 1) * 8 + 1\n",
    "\n",
    "height = ((height - 1) // 32 + 1) * 32\n",
    "width = ((width - 1) // 32 + 1) * 32\n",
    "\n",
    "latent_frame_rate = frame_rate / video_scale_factor\n",
    "\n",
    "\n",
    "latent_frame_rates = (\n",
    "                    torch.ones(\n",
    "                        2, 1, device=\"cuda:0\"\n",
    "                    )\n",
    "                    * latent_frame_rate\n",
    "                )\n",
    "\n",
    "latent_height = height // vae_scale_factor\n",
    "latent_width = width // vae_scale_factor\n",
    "latent_num_frames = num_frames // video_scale_factor + 1\n",
    "num_latent_patches = latent_height * latent_width * latent_num_frames\n",
    "\n",
    "def append_dims(x: torch.Tensor, target_dims: int) -> torch.Tensor:\n",
    "    \"\"\"Appends dimensions to the end of a tensor until it has target_dims dimensions.\"\"\"\n",
    "    dims_to_append = target_dims - x.ndim\n",
    "    if dims_to_append < 0:\n",
    "        raise ValueError(\n",
    "            f\"input has {x.ndim} dims but target_dims is {target_dims}, which is less\"\n",
    "        )\n",
    "    elif dims_to_append == 0:\n",
    "        return x\n",
    "    return x[(...,) + (None,) * dims_to_append]\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    input_tensor: torch.Tensor,\n",
    "    freqs_cis: Tuple[torch.FloatTensor, torch.FloatTensor],\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    cos_freqs = freqs_cis[0]\n",
    "    sin_freqs = freqs_cis[1]\n",
    "\n",
    "    t_dup = rearrange(input_tensor, \"... (d r) -> ... d r\", r=2)\n",
    "    t1, t2 = t_dup.unbind(dim=-1)\n",
    "    t_dup = torch.stack((-t2, t1), dim=-1)\n",
    "    input_tensor_rot = rearrange(t_dup, \"... d r -> ... (d r)\")\n",
    "    \n",
    "    out = input_tensor * cos_freqs + input_tensor_rot * sin_freqs\n",
    "\n",
    "    return out\n",
    "def get_grid(\n",
    "        orig_num_frames, orig_height, orig_width, batch_size, scale_grid, device\n",
    "    ):\n",
    "        _patch_size = [1, 1, 1]\n",
    "        f = orig_num_frames // _patch_size[0]\n",
    "        h = orig_height // _patch_size[1]\n",
    "        w = orig_width // _patch_size[2]\n",
    "        grid_h = torch.arange(h, dtype=torch.float32, device=device)\n",
    "        grid_w = torch.arange(w, dtype=torch.float32, device=device)\n",
    "        grid_f = torch.arange(f, dtype=torch.float32, device=device)\n",
    "        grid = torch.meshgrid(grid_f, grid_h, grid_w)\n",
    "        grid = torch.stack(grid, dim=0)\n",
    "        grid = grid.unsqueeze(0).repeat(batch_size, 1, 1, 1, 1)\n",
    "\n",
    "        if scale_grid is not None:\n",
    "            for i in range(3):\n",
    "                if isinstance(scale_grid[i], torch.Tensor):\n",
    "                    scale = append_dims(scale_grid[i], grid.ndim - 1)\n",
    "                else:\n",
    "                    scale = scale_grid[i]\n",
    "                grid[:, i, ...] = grid[:, i, ...] * scale * _patch_size[i]\n",
    "\n",
    "        grid = rearrange(grid, \"b c f h w -> b c (f h w)\", b=batch_size)\n",
    "        return grid\n",
    "\n",
    "scale_grid = ((\n",
    "    1 / latent_frame_rates,\n",
    "    vae_scale_factor,\n",
    "    vae_scale_factor,\n",
    "))\n",
    "\n",
    "indices_grid = get_grid(\n",
    "                    orig_num_frames=latent_num_frames,\n",
    "                    orig_height=latent_height,\n",
    "                    orig_width=latent_width,\n",
    "                    batch_size=2,\n",
    "                    scale_grid=scale_grid,\n",
    "                    device=\"cuda\",\n",
    ")\n",
    "\n",
    "def get_fractional_positions(indices_grid):\n",
    "    fractional_positions = torch.stack(\n",
    "        [\n",
    "            indices_grid[:, i] / [20, 2048, 2048][i]\n",
    "            for i in range(3)\n",
    "        ],\n",
    "        dim=-1,\n",
    "    )\n",
    "    return fractional_positions\n",
    "\n",
    "def precompute_freqs_cis( indices_grid, spacing=\"exp\"):\n",
    "    dtype = torch.float32  # We need full precision in the freqs_cis computation.\n",
    "    dim = 2048\n",
    "    theta = 10000.0\n",
    "\n",
    "    fractional_positions = get_fractional_positions(indices_grid)\n",
    "\n",
    "    start = 1\n",
    "    end = theta\n",
    "    device = fractional_positions.device\n",
    "    if spacing == \"exp\":\n",
    "        indices = theta ** (\n",
    "            torch.linspace(\n",
    "                math.log(start, theta),\n",
    "                math.log(end, theta),\n",
    "                dim // 6,\n",
    "                device=device,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "        )\n",
    "        indices = indices.to(dtype=dtype)\n",
    "    elif spacing == \"exp_2\":\n",
    "        indices = 1.0 / theta ** (torch.arange(0, dim, 6, device=device) / dim)\n",
    "        indices = indices.to(dtype=dtype)\n",
    "    elif spacing == \"linear\":\n",
    "        indices = torch.linspace(start, end, dim // 6, device=device, dtype=dtype)\n",
    "    elif spacing == \"sqrt\":\n",
    "        indices = torch.linspace(\n",
    "            start**2, end**2, dim // 6, device=device, dtype=dtype\n",
    "        ).sqrt()\n",
    "\n",
    "    indices = indices * math.pi / 2\n",
    "\n",
    "    if spacing == \"exp_2\":\n",
    "        freqs = (\n",
    "            (indices * fractional_positions.unsqueeze(-1))\n",
    "            .transpose(-1, -2)\n",
    "            .flatten(2)\n",
    "        )\n",
    "    else:\n",
    "        freqs = (\n",
    "            (indices * (fractional_positions.unsqueeze(-1) * 2 - 1))\n",
    "            .transpose(-1, -2)\n",
    "            .flatten(2)\n",
    "        )\n",
    "\n",
    "    cos_freq = freqs.cos().repeat_interleave(2, dim=-1)\n",
    "    sin_freq = freqs.sin().repeat_interleave(2, dim=-1)\n",
    "    if dim % 6 != 0:\n",
    "        cos_padding = torch.ones_like(cos_freq[:, :, : dim % 6])\n",
    "        sin_padding = torch.zeros_like(cos_freq[:, :, : dim % 6])\n",
    "        cos_freq = torch.cat([cos_padding, cos_freq], dim=-1)\n",
    "        sin_freq = torch.cat([sin_padding, sin_freq], dim=-1)\n",
    "    return cos_freq.to(dtype), sin_freq.to(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_freq, sin_freq = precompute_freqs_cis(indices_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_idx = 0\n",
    "x = torch.load(f\"/data/LTXVideo/acts/ffn/hs-{l_idx}.pt\", map_location=\"cuda\")[:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3795, 2048])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_freq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.04640007019043\n"
     ]
    }
   ],
   "source": [
    "x_fp8 = x.to(torch.float8_e4m3fn)\n",
    "s = torch.cuda.Event(True)\n",
    "e = torch.cuda.Event(True)\n",
    "s.record()\n",
    "o_rope = rope(x_fp8, cos_freq, sin_freq)\n",
    "e.record()\n",
    "torch.cuda.synchronize()\n",
    "print(s.elapsed_time(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.49785614013672\n"
     ]
    }
   ],
   "source": [
    "cos_freq_fp16 = cos_freq.to(torch.bfloat16)\n",
    "sin_freq_fp16 = sin_freq.to(torch.bfloat16)\n",
    "freqs = [cos_freq_fp16, sin_freq_fp16]\n",
    "torch.cuda.synchronize()\n",
    "s = torch.cuda.Event(True)\n",
    "e = torch.cuda.Event(True)\n",
    "s.record()\n",
    "o_torch_rope = apply_rotary_emb(x, freqs)\n",
    "e.record()\n",
    "torch.cuda.synchronize()\n",
    "print(s.elapsed_time(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_freq.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_torch_rope_ref = apply_rotary_emb(x.float(), [cos_freq, sin_freq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9727, device='cuda:0')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(o_rope.float() - o_torch_rope_ref.float()).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(o_rope.float() - o_torch_rope.float()).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0840, device='cuda:0')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(o_torch_rope.float() - o_torch_rope_ref.float()).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0163, device='cuda:0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def diff_rms(a, b):\n",
    "    return torch.sqrt(((a.float() - b.float()).square().sum()/a.numel()))\n",
    "\n",
    "diff_rms(o_rope, o_torch_rope.to(torch.float8_e4m3fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0106, device='cuda:0')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_rms(o_torch_rope_ref, o_torch_rope.to(torch.float8_e4m3fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0139, device='cuda:0')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_rms(o_torch_rope_ref, o_rope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_rms(o_torch_rope_ref, o_torch_rope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0020, 0.0049, 0.0098], device='cuda:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.quantile((o_rope.float() - o_torch_rope.float()).abs(), torch.tensor([0.25, 0.5, 0.75, 0.9]).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
