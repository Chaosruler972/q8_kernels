{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q8_matmul.gemm._C import q8_mm\n",
    "from q8_matmul.quantizer._C import tokenwise_quant\n",
    "from q8_matmul.ops._C import rms_norm\n",
    "\n",
    "from fast_hadamard_transform import hadamard_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fast_hadamard_transform import hadamard_transform\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "def hadamard_quant(x):\n",
    "    k = x.shape[-1]\n",
    "    x_hadamard = hadamard_transform(x, scale=1/math.sqrt(k))\n",
    "    x_abs_max_hadamard = x_hadamard.float().abs().max(-1, False).values\n",
    "    x_scale_hadamard = x_abs_max_hadamard/127.0\n",
    "    x_q8_hadamard = (x_hadamard.float() / x_scale_hadamard[..., None]).round().to(torch.int8)\n",
    "    return x_q8_hadamard, x_scale_hadamard\n",
    "\n",
    "def quant(x):\n",
    "    x_abs_max = x.float().abs().max(-1, False).values\n",
    "    x_scale = x_abs_max/127.0\n",
    "    x_q8 =  (x.float() / x_scale[..., None]).round().to(torch.int8)\n",
    "    return x_q8, x_scale\n",
    "\n",
    "\n",
    "l_idx = 3\n",
    "x = torch.load(f\"/data/LTXVideo/acts/ffn/hs-{l_idx}.pt\", map_location=\"cuda\")[:, :, :]\n",
    "model_weights = load_file(\"/data/ltx_weights/unet/unet_diffusion_pytorch_model.safetensors\", device=\"cpu\")\n",
    "w = model_weights[f\"transformer_blocks.{l_idx}.ff.net.0.proj.weight\"].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weights[\"transformer_blocks.15.ff.net.0.proj.bias\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3795, 2048])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_blocks.0.attn1.k_norm.weight\n",
      "transformer_blocks.0.attn1.q_norm.weight\n",
      "transformer_blocks.0.attn2.k_norm.weight\n",
      "transformer_blocks.0.attn2.q_norm.weight\n",
      "transformer_blocks.1.attn1.k_norm.weight\n",
      "transformer_blocks.1.attn1.q_norm.weight\n",
      "transformer_blocks.1.attn2.k_norm.weight\n",
      "transformer_blocks.1.attn2.q_norm.weight\n",
      "transformer_blocks.10.attn1.k_norm.weight\n",
      "transformer_blocks.10.attn1.q_norm.weight\n",
      "transformer_blocks.10.attn2.k_norm.weight\n",
      "transformer_blocks.10.attn2.q_norm.weight\n",
      "transformer_blocks.11.attn1.k_norm.weight\n",
      "transformer_blocks.11.attn1.q_norm.weight\n",
      "transformer_blocks.11.attn2.k_norm.weight\n",
      "transformer_blocks.11.attn2.q_norm.weight\n",
      "transformer_blocks.12.attn1.k_norm.weight\n",
      "transformer_blocks.12.attn1.q_norm.weight\n",
      "transformer_blocks.12.attn2.k_norm.weight\n",
      "transformer_blocks.12.attn2.q_norm.weight\n",
      "transformer_blocks.13.attn1.k_norm.weight\n",
      "transformer_blocks.13.attn1.q_norm.weight\n",
      "transformer_blocks.13.attn2.k_norm.weight\n",
      "transformer_blocks.13.attn2.q_norm.weight\n",
      "transformer_blocks.14.attn1.k_norm.weight\n",
      "transformer_blocks.14.attn1.q_norm.weight\n",
      "transformer_blocks.14.attn2.k_norm.weight\n",
      "transformer_blocks.14.attn2.q_norm.weight\n",
      "transformer_blocks.15.attn1.k_norm.weight\n",
      "transformer_blocks.15.attn1.q_norm.weight\n",
      "transformer_blocks.15.attn2.k_norm.weight\n",
      "transformer_blocks.15.attn2.q_norm.weight\n",
      "transformer_blocks.16.attn1.k_norm.weight\n",
      "transformer_blocks.16.attn1.q_norm.weight\n",
      "transformer_blocks.16.attn2.k_norm.weight\n",
      "transformer_blocks.16.attn2.q_norm.weight\n",
      "transformer_blocks.17.attn1.k_norm.weight\n",
      "transformer_blocks.17.attn1.q_norm.weight\n",
      "transformer_blocks.17.attn2.k_norm.weight\n",
      "transformer_blocks.17.attn2.q_norm.weight\n",
      "transformer_blocks.18.attn1.k_norm.weight\n",
      "transformer_blocks.18.attn1.q_norm.weight\n",
      "transformer_blocks.18.attn2.k_norm.weight\n",
      "transformer_blocks.18.attn2.q_norm.weight\n",
      "transformer_blocks.19.attn1.k_norm.weight\n",
      "transformer_blocks.19.attn1.q_norm.weight\n",
      "transformer_blocks.19.attn2.k_norm.weight\n",
      "transformer_blocks.19.attn2.q_norm.weight\n",
      "transformer_blocks.2.attn1.k_norm.weight\n",
      "transformer_blocks.2.attn1.q_norm.weight\n",
      "transformer_blocks.2.attn2.k_norm.weight\n",
      "transformer_blocks.2.attn2.q_norm.weight\n",
      "transformer_blocks.20.attn1.k_norm.weight\n",
      "transformer_blocks.20.attn1.q_norm.weight\n",
      "transformer_blocks.20.attn2.k_norm.weight\n",
      "transformer_blocks.20.attn2.q_norm.weight\n",
      "transformer_blocks.21.attn1.k_norm.weight\n",
      "transformer_blocks.21.attn1.q_norm.weight\n",
      "transformer_blocks.21.attn2.k_norm.weight\n",
      "transformer_blocks.21.attn2.q_norm.weight\n",
      "transformer_blocks.22.attn1.k_norm.weight\n",
      "transformer_blocks.22.attn1.q_norm.weight\n",
      "transformer_blocks.22.attn2.k_norm.weight\n",
      "transformer_blocks.22.attn2.q_norm.weight\n",
      "transformer_blocks.23.attn1.k_norm.weight\n",
      "transformer_blocks.23.attn1.q_norm.weight\n",
      "transformer_blocks.23.attn2.k_norm.weight\n",
      "transformer_blocks.23.attn2.q_norm.weight\n",
      "transformer_blocks.24.attn1.k_norm.weight\n",
      "transformer_blocks.24.attn1.q_norm.weight\n",
      "transformer_blocks.24.attn2.k_norm.weight\n",
      "transformer_blocks.24.attn2.q_norm.weight\n",
      "transformer_blocks.25.attn1.k_norm.weight\n",
      "transformer_blocks.25.attn1.q_norm.weight\n",
      "transformer_blocks.25.attn2.k_norm.weight\n",
      "transformer_blocks.25.attn2.q_norm.weight\n",
      "transformer_blocks.26.attn1.k_norm.weight\n",
      "transformer_blocks.26.attn1.q_norm.weight\n",
      "transformer_blocks.26.attn2.k_norm.weight\n",
      "transformer_blocks.26.attn2.q_norm.weight\n",
      "transformer_blocks.27.attn1.k_norm.weight\n",
      "transformer_blocks.27.attn1.q_norm.weight\n",
      "transformer_blocks.27.attn2.k_norm.weight\n",
      "transformer_blocks.27.attn2.q_norm.weight\n",
      "transformer_blocks.3.attn1.k_norm.weight\n",
      "transformer_blocks.3.attn1.q_norm.weight\n",
      "transformer_blocks.3.attn2.k_norm.weight\n",
      "transformer_blocks.3.attn2.q_norm.weight\n",
      "transformer_blocks.4.attn1.k_norm.weight\n",
      "transformer_blocks.4.attn1.q_norm.weight\n",
      "transformer_blocks.4.attn2.k_norm.weight\n",
      "transformer_blocks.4.attn2.q_norm.weight\n",
      "transformer_blocks.5.attn1.k_norm.weight\n",
      "transformer_blocks.5.attn1.q_norm.weight\n",
      "transformer_blocks.5.attn2.k_norm.weight\n",
      "transformer_blocks.5.attn2.q_norm.weight\n",
      "transformer_blocks.6.attn1.k_norm.weight\n",
      "transformer_blocks.6.attn1.q_norm.weight\n",
      "transformer_blocks.6.attn2.k_norm.weight\n",
      "transformer_blocks.6.attn2.q_norm.weight\n",
      "transformer_blocks.7.attn1.k_norm.weight\n",
      "transformer_blocks.7.attn1.q_norm.weight\n",
      "transformer_blocks.7.attn2.k_norm.weight\n",
      "transformer_blocks.7.attn2.q_norm.weight\n",
      "transformer_blocks.8.attn1.k_norm.weight\n",
      "transformer_blocks.8.attn1.q_norm.weight\n",
      "transformer_blocks.8.attn2.k_norm.weight\n",
      "transformer_blocks.8.attn2.q_norm.weight\n",
      "transformer_blocks.9.attn1.k_norm.weight\n",
      "transformer_blocks.9.attn1.q_norm.weight\n",
      "transformer_blocks.9.attn2.k_norm.weight\n",
      "transformer_blocks.9.attn2.q_norm.weight\n"
     ]
    }
   ],
   "source": [
    "for k in model_weights:\n",
    "    if \"norm\" in k:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_weights = model_weights[\"transformer_blocks.0.attn2.k_norm.weight\"].cuda().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0258, 0.0448, 0.0280,  ..., 0.0207, 0.0159, 0.0105], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3795, 2048])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.309119939804077\n"
     ]
    }
   ],
   "source": [
    "x_fp8 = x.to(torch.float8_e4m3fn)\n",
    "s = torch.cuda.Event(True)\n",
    "e = torch.cuda.Event(True)\n",
    "s.record()\n",
    "x_normed = rms_norm(x_fp8, norm_weights)\n",
    "e.record()\n",
    "torch.cuda.synchronize()\n",
    "print(s.elapsed_time(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_weights.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_weights_fp16 = norm_weights.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.932159423828125\n"
     ]
    }
   ],
   "source": [
    "x_normed_torch = (norm_weights_fp16 * (x * torch.rsqrt(x.float().pow(2).mean(-1, keepdim=True)).to(torch.bfloat16)))\n",
    "torch.cuda.synchronize()\n",
    "s = torch.cuda.Event(True)\n",
    "e = torch.cuda.Event(True)\n",
    "s.record()\n",
    "x_normed_torch = (norm_weights_fp16 * (x * torch.rsqrt(x.float().pow(2).mean(-1, keepdim=True)).to(torch.bfloat16)))\n",
    "e.record()\n",
    "torch.cuda.synchronize()\n",
    "print(s.elapsed_time(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_normed_torch.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3795, 2048])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_fp8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0059, -0.0117,  0.0039,  ...,  0.0176, -0.0039,  0.0117],\n",
       "         [ 0.0117, -0.0293,  0.0000,  ...,  0.0176, -0.0078,  0.0078],\n",
       "         [ 0.0156, -0.0352, -0.0020,  ...,  0.0215,  0.0039,  0.0078],\n",
       "         ...,\n",
       "         [-0.0195, -0.0234,  0.0117,  ...,  0.0020, -0.0020, -0.0020],\n",
       "         [-0.0117, -0.0215,  0.0039,  ...,  0.0059, -0.0020,  0.0059],\n",
       "         [-0.0078, -0.0430,  0.0020,  ...,  0.0059, -0.0059,  0.0039]],\n",
       "\n",
       "        [[ 0.0020, -0.0195,  0.0020,  ...,  0.0195, -0.0039,  0.0098],\n",
       "         [ 0.0098, -0.0352, -0.0039,  ...,  0.0215, -0.0078,  0.0078],\n",
       "         [ 0.0117, -0.0430, -0.0059,  ...,  0.0254,  0.0020,  0.0078],\n",
       "         ...,\n",
       "         [-0.0195, -0.0137,  0.0098,  ...,  0.0039, -0.0020, -0.0020],\n",
       "         [-0.0117, -0.0156,  0.0059,  ...,  0.0059, -0.0039,  0.0039],\n",
       "         [-0.0059, -0.0391,  0.0039,  ...,  0.0078, -0.0059,  0.0020]]],\n",
       "       device='cuda:0', dtype=torch.float8_e4m3fn)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_normed_torch = (norm_weights * x.to(torch.float8_e4m3fn).float() * torch.rsqrt(x_fp8.float().pow(2).mean(-1, keepdim=True))).to(torch.float8_e4m3fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0156, device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_normed_torch.float() - x_normed.float()).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
       "       dtype=torch.float8_e4m3fn)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_blocks.0.attn1.k_norm.weight\n",
      "transformer_blocks.0.attn1.q_norm.weight\n",
      "transformer_blocks.0.attn1.to_k.bias\n",
      "transformer_blocks.0.attn1.to_k.weight\n",
      "transformer_blocks.0.attn1.to_out.0.bias\n",
      "transformer_blocks.0.attn1.to_out.0.weight\n",
      "transformer_blocks.0.attn1.to_q.bias\n",
      "transformer_blocks.0.attn1.to_q.weight\n",
      "transformer_blocks.0.attn1.to_v.bias\n",
      "transformer_blocks.0.attn1.to_v.weight\n",
      "transformer_blocks.0.attn2.k_norm.weight\n",
      "transformer_blocks.0.attn2.q_norm.weight\n",
      "transformer_blocks.0.attn2.to_k.bias\n",
      "transformer_blocks.0.attn2.to_k.weight\n",
      "transformer_blocks.0.attn2.to_out.0.bias\n",
      "transformer_blocks.0.attn2.to_out.0.weight\n",
      "transformer_blocks.0.attn2.to_q.bias\n",
      "transformer_blocks.0.attn2.to_q.weight\n",
      "transformer_blocks.0.attn2.to_v.bias\n",
      "transformer_blocks.0.attn2.to_v.weight\n",
      "transformer_blocks.0.ff.net.0.proj.bias\n",
      "transformer_blocks.0.ff.net.0.proj.weight\n",
      "transformer_blocks.0.ff.net.2.bias\n",
      "transformer_blocks.0.ff.net.2.weight\n",
      "transformer_blocks.0.scale_shift_table\n"
     ]
    }
   ],
   "source": [
    "for k in model_weights:\n",
    "    if \"transformer_blocks.0\" in k:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = x.shape[-1]\n",
    "x_hadamard = hadamard_transform(x.to(torch.float8_e4m3fn), scale=1/math.sqrt(k))\n",
    "w_hadamard = hadamard_transform(w.to(torch.float8_e4m3fn), scale=1/math.sqrt(k))\n",
    "\n",
    "x_quant_h, x_scales_h = hadamard_quant(x.to(torch.float8_e4m3fn))\n",
    "w_quant_h, w_scales_h = hadamard_quant(w.to(torch.float8_e4m3fn))\n",
    "\n",
    "x_quant, x_scales = quant(x.to(torch.float8_e4m3fn))\n",
    "w_quant, w_scales = quant(w.to(torch.float8_e4m3fn))\n",
    "\n",
    "\n",
    "x_quant_h, x_scales_h = tokenwise_quant(x_hadamard)\n",
    "w_quant_h, w_scales_h = tokenwise_quant(w_hadamard)\n",
    "\n",
    "x_quant, x_scales = tokenwise_quant(x.to(torch.float8_e4m3fn))\n",
    "w_quant, w_scales = tokenwise_quant(w.to(torch.float8_e4m3fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_q8_h = q8_mm(x_quant_h[0].contiguous(), w_quant_h, x_scales_h, w_scales_h, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scales(scales_tensor, ix, idx, type=\"A\"):\n",
    "    block = scales_tensor[ix*128:(ix+1)*128]\n",
    "    warp = idx // 32\n",
    "    if type == \"A\":\n",
    "        row = warp % 2 * 16 + idx // 4\n",
    "        arr = []\n",
    "        for i in range(4*4):\n",
    "            v = i % 4\n",
    "            v_row = v // 2 * 8\n",
    "            mma_m = i // 4\n",
    "            arr.append(block[row+v_row + mma_m*32].item())\n",
    "        print(arr)\n",
    "    else:\n",
    "        col = warp // 2 * 8 + (idx % 4) * 2\n",
    "        arr = []\n",
    "        for i in range(4*8):\n",
    "            v = i % 4\n",
    "            v_col = v % 2\n",
    "            mma_n = i // 4\n",
    "            arr.append(block[col+v_col + mma_n*16].item())\n",
    "        print(arr)\n",
    "# print_scales(w_scales_h, 8, 2, \"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3795, 8192])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_q8_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_q8_h = q8_mm(x_quant_h, w_quant_h, x_scales_h, w_scales_h, False)\n",
    "o_q8 = q8_mm(x_quant, w_quant, x_scales, w_scales, False)\n",
    "\n",
    "o_q8_torch_h = ((x_scales_h[..., None] * w_scales_h[None, None, :]) * torch.matmul(x_quant_h.float(), w_quant_h.float().t())).to(torch.float8_e4m3fn)\n",
    "o_q8_torch = ((x_scales[..., None] * w_scales[None, None, :]) * torch.matmul(x_quant.float(), w_quant.float().t())).to(torch.float8_e4m3fn)\n",
    "\n",
    "\n",
    "o_orig = torch.matmul(x.half(), w.half().t())# nn.functional.linear(x, w, bias=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_max(a, b):\n",
    "    return (a.float() - b.float()).abs().max()\n",
    "\n",
    "def diff_mean(a, b):\n",
    "    return (a.float() - b.float()).abs().mean()\n",
    "\n",
    "\n",
    "def diff_quantiles(a, b):\n",
    "    if a.ndim > 2:\n",
    "        return torch.quantile((a.float() - b.float()).abs()[1, :2048, :], torch.tensor([0.25, 0.5, 0.75, 0.9, 0.99, 1.0]).cuda())\n",
    "    else:\n",
    "        return torch.quantile((a.float() - b.float()).abs()[:2048, :], torch.tensor([0.25, 0.5, 0.75, 0.9, 0.99, 1.0]).cuda())\n",
    "        \n",
    "diff_q8_h = diff_max(o_q8_h, o_orig)\n",
    "diff_q8 = diff_max(o_q8, o_orig)\n",
    "diff_q8_torch_h = diff_max(o_q8_torch_h, o_orig)\n",
    "diff_q8_torch = diff_max(o_q8_torch, o_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_fp8_orig = torch._scaled_mm(x[1].to(torch.float8_e4m3fn),  w.to(torch.float8_e4m3fn).contiguous().t(), scale_a=torch.tensor([1.0]).cuda(), scale_b=torch.tensor([1.0]).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIFF Hadamard:  tensor(0.5859, device='cuda:0')\n",
      "DIFF no Hadamard:  tensor(0.7031, device='cuda:0')\n",
      "DIFF Hadamard Torch:  tensor(0.5859, device='cuda:0')\n",
      "DIFF no Hadamard Torch:  tensor(0.7031, device='cuda:0')\n",
      "Diff mean hadamard:  tensor(0.5859, device='cuda:0')\n",
      "Diff mean:  tensor(0.7031, device='cuda:0')\n",
      "torch q8:  tensor(0., device='cuda:0')\n",
      "torch fp8:  tensor(0.4609, device='cuda:0')\n",
      "diff torch fp8 q8:  tensor(1., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"DIFF Hadamard: \", diff_q8_h)\n",
    "print(\"DIFF no Hadamard: \", diff_q8)\n",
    "\n",
    "print(\"DIFF Hadamard Torch: \", diff_q8_torch_h)\n",
    "print(\"DIFF no Hadamard Torch: \", diff_q8_torch)\n",
    "\n",
    "print(\"Diff mean hadamard: \", diff_max(o_orig, o_q8_h))\n",
    "print(\"Diff mean: \", diff_max(o_orig, o_q8))\n",
    "\n",
    "print(\"torch q8: \", diff_max(o_q8_torch_h, o_q8_h))\n",
    "print(\"torch fp8: \", diff_max(o_fp8_orig, o_orig[1]))\n",
    "\n",
    "print(\"diff torch fp8 q8: \", diff_max(o_fp8_orig, o_q8_h[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIFF Hadamard:  tensor(0.5859, device='cuda:0')\n",
    "# DIFF no Hadamard:  tensor(0.7031, device='cuda:0')\n",
    "# DIFF Hadamard Torch:  tensor(0.5859, device='cuda:0')\n",
    "# DIFF no Hadamard Torch:  tensor(0.7031, device='cuda:0')\n",
    "# Diff mean hadamard:  tensor(0.5859, device='cuda:0')\n",
    "# Diff mean:  tensor(0.7031, device='cuda:0')\n",
    "# torch q8:  tensor(0., device='cuda:0')\n",
    "# torch fp8:  tensor(0.4609, device='cuda:0')\n",
    "# diff torch fp8 q8:  tensor(1., device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3795, 8192])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_q8_torch_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3795, 8192])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_fp8_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0029, device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((o_q8_h[1].float() * o_orig[1].float()) < 0).sum()/o_orig[1].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0044, device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((o_q8.float() * o_orig.float()) < 0).sum()/o_orig.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0009, device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((o_fp8_orig.float() * o_orig[1].float()) < 0).sum()/o_orig.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2113, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_orig[(o_q8_h.float() * o_orig.float()) < 0].abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2188, device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_q8_h.float()[(o_q8_h.float() * o_orig.float()) < 0].abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1755, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_orig[1][(o_fp8_orig.float() * o_orig[1].float()) < 0].abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0210, 0.0454, 0.0801, 0.1113, 0.1914, 0.4102], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_quantiles(o_orig, o_q8_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0137, 0.0381, 0.0762, 0.1074, 0.1875, 0.3594], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_quantiles(o_orig[1], o_fp8_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = x.shape[0]\n",
    "m = x.shape[1]\n",
    "n = w.shape[0]\n",
    "k = x.shape[-1]\n",
    "int8_tflops = []\n",
    "TFLOPS = 2 * batch * m *n *k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109.99504667097293\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.synchronize()\n",
    "N_ROUNDS = 10\n",
    "N_OUTER_ROUNDS = 1\n",
    "\n",
    "for _ in range(5):\n",
    "    o_q8_h = q8_mm(x_quant_h, w_quant_h, x_scales_h, w_scales_h, True)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "for _ in range(N_OUTER_ROUNDS):\n",
    "    start_events = [ torch.cuda.Event(True) for _ in range(N_ROUNDS)]\n",
    "    end_events = [ torch.cuda.Event(True) for _ in range(N_ROUNDS)]\n",
    "    for i in range(N_ROUNDS):\n",
    "        start_events[i].record()\n",
    "        o_q8_h = q8_mm(x_quant_h, w_quant_h, x_scales_h, w_scales_h, True)\n",
    "        end_events[i].record()\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed_times = [s.elapsed_time(e) for s, e in zip(start_events, end_events)]\n",
    "    int8_tflops.append((TFLOPS * 1e-12)/(min(elapsed_times) * 1e-3))\n",
    "print(max(int8_tflops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp8_tflops = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.16396682309988\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.synchronize()\n",
    "N_ROUNDS = 10\n",
    "N_OUTER_ROUNDS = 1\n",
    "_w = w_hadamard.contiguous().t()\n",
    "_scale = torch.tensor([1.0]).cuda()\n",
    "for _ in range(5):\n",
    "    o_fp8_orig = torch._scaled_mm(x_hadamard[0],  _w, scale_a=_scale, scale_b=_scale)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "for _ in range(N_OUTER_ROUNDS):\n",
    "    start_events = [ torch.cuda.Event(True) for _ in range(N_ROUNDS)]\n",
    "    end_events = [ torch.cuda.Event(True) for _ in range(N_ROUNDS)]\n",
    "    for i in range(N_ROUNDS):\n",
    "        start_events[i].record()\n",
    "        o_fp8_orig = torch._scaled_mm(x_hadamard[0],  _w, scale_a=torch.tensor([1.0]).cuda(), scale_b=torch.tensor([1.0]).cuda())\n",
    "        end_events[i].record()\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed_times = [s.elapsed_time(e) for s, e in zip(start_events, end_events)]\n",
    "    fp8_tflops.append((TFLOPS/2 * 1e-12)/(min(elapsed_times) * 1e-3))\n",
    "print(max(fp8_tflops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT8/FP8:  1.8911200985571068\n"
     ]
    }
   ],
   "source": [
    "print(\"INT8/FP8: \", max(int8_tflops)/max(fp8_tflops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp16_tflops = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.924014044917687\n"
     ]
    }
   ],
   "source": [
    "# for _ in range(10):\n",
    "#     o = flash_attention_int8(q_quant, k_quant, v_quant, q_scales, k_scales, v_scales)\n",
    "# flash_attention_int8_4stages\n",
    "torch.cuda.synchronize()\n",
    "N_ROUNDS = 10\n",
    "N_OUTER_ROUNDS = 1\n",
    "\n",
    "a_half = x.half()\n",
    "b_half = w.half().t().contiguous()\n",
    "\n",
    "for _ in range(5):\n",
    "    o_half = torch.matmul(a_half, b_half)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "for _ in range(N_OUTER_ROUNDS):\n",
    "    start_events = [ torch.cuda.Event(True) for _ in range(N_ROUNDS)]\n",
    "    end_events = [ torch.cuda.Event(True) for _ in range(N_ROUNDS)]\n",
    "    for i in range(N_ROUNDS):\n",
    "        start_events[i].record()\n",
    "        o_half = torch.matmul(a_half, b_half)\n",
    "        end_events[i].record()\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed_times = [s.elapsed_time(e) for s, e in zip(start_events, end_events)]\n",
    "    fp16_tflops.append((TFLOPS * 1e-12)/(min(elapsed_times) * 1e-3))\n",
    "print(max(fp16_tflops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from attention_cutlass_fp8 import flash_attention_fp8_even, flash_attention_fp8\n",
    "from fast_hadamard_transform import hadamard_transform\n",
    "\n",
    "l = 12\n",
    "q = torch.load(f\"/data/LTXVideo/acts/attn/q-{l}.pt\")[:, :, :].cuda().contiguous().half()\n",
    "k = torch.load(f\"/data/LTXVideo/acts/attn/k-{l}.pt\")[:, :, :].cuda().contiguous().half()\n",
    "v = torch.load(f\"/data/LTXVideo/acts/attn/v-{l}.pt\")[:, :, :].cuda().contiguous().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 3795, 64])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_dim = q.shape[-1]\n",
    "sm_scale = 1/math.sqrt(head_dim)\n",
    "sm_scale_fp8 = sm_scale*1.44269504\n",
    "\n",
    "q_fp8 = q.to(torch.float8_e4m3fn)\n",
    "k_fp8 = k.to(torch.float8_e4m3fn)\n",
    "v_fp8 = v.transpose(2, 3).contiguous().to(torch.float8_e4m3fn)\n",
    "\n",
    "q_hadamard = hadamard_transform(q, scale=1/math.sqrt(head_dim)).to(torch.float8_e4m3fn)\n",
    "k_hadamard = hadamard_transform(k, scale=1/math.sqrt(head_dim)).to(torch.float8_e4m3fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.720255851745605\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "s = torch.cuda.Event(True)\n",
    "e = torch.cuda.Event(True)\n",
    "v_tokens = v.shape[-2]\n",
    "v_tokens_pad = ((v_tokens + 15)//16)*16 - v_tokens\n",
    "s.record()\n",
    "v_fp8_padded = torch.nn.functional.pad(v_fp8, (0, v_tokens_pad))\n",
    "e.record()\n",
    "torch.cuda.synchronize()\n",
    "print(s.elapsed_time(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFLOPS_PER_ATTN = 4*q.shape[0]*q.shape[1]*q.shape[2]*q.shape[2]*q.shape[3]\n",
    "int8_tflops = []\n",
    "fp16_tflops = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.8809854556419\n"
     ]
    }
   ],
   "source": [
    "# for _ in range(10):\n",
    "#     o = flash_attention_int8(q_quant, k_quant, v_quant, q_scales, k_scales, v_scales)\n",
    "# flash_attention_int8_4stages\n",
    "torch.cuda.synchronize()\n",
    "N_ROUNDS = 10\n",
    "N_OUTER_ROUNDS = 1\n",
    "sm_scale_fp8 =sm_scale*1.44269504\n",
    "for _ in range(5):\n",
    "    o = flash_attention_fp8_even(q_fp8, k_fp8, v_fp8_padded, sm_scale_fp8)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "for _ in range(N_OUTER_ROUNDS):\n",
    "    start_events = [ torch.cuda.Event(True) for _ in range(N_ROUNDS)]\n",
    "    end_events = [ torch.cuda.Event(True) for _ in range(N_ROUNDS)]\n",
    "    for i in range(N_ROUNDS):\n",
    "        start_events[i].record()\n",
    "        v_fp8_padded = torch.nn.functional.pad(v_fp8, (0, v_tokens_pad))\n",
    "        o = flash_attention_fp8_even(q_fp8, k_fp8, v_fp8_padded, sm_scale_fp8)\n",
    "        end_events[i].record()\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed_times = [s.elapsed_time(e) for s, e in zip(start_events, end_events)]\n",
    "    int8_tflops.append((TFLOPS_PER_ATTN * 1e-12)/(min(elapsed_times) * 1e-3))\n",
    "print(max(int8_tflops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.397952079772949"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(elapsed_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.043372120269883\n"
     ]
    }
   ],
   "source": [
    "# for _ in range(10):\n",
    "#     o = flash_attention_int8(q_quant, k_quant, v_quant, q_scales, k_scales, v_scales)\n",
    "# flash_attention_int8_4stages\n",
    "torch.cuda.synchronize()\n",
    "N_ROUNDS = 10\n",
    "N_OUTER_ROUNDS = 1\n",
    "for _ in range(5):\n",
    "    o_half = torch.nn.functional.scaled_dot_product_attention(q, k, v, scale=sm_scale)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "for _ in range(N_OUTER_ROUNDS):\n",
    "    start_events = [ torch.cuda.Event(True) for _ in range(N_ROUNDS)]\n",
    "    end_events = [ torch.cuda.Event(True) for _ in range(N_ROUNDS)]\n",
    "    for i in range(N_ROUNDS):\n",
    "        start_events[i].record()\n",
    "        \n",
    "        o_half = torch.nn.functional.scaled_dot_product_attention(q, k, v, scale=sm_scale)\n",
    "\n",
    "        end_events[i].record()\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed_times = [s.elapsed_time(e) for s, e in zip(start_events, end_events)]\n",
    "    fp16_tflops.append((TFLOPS_PER_ATTN * 1e-12)/(min(elapsed_times) * 1e-3))\n",
    "print(max(fp16_tflops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 64, 3795])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_fp8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7772160, 242880, 3795, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_fp8.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_fp8 = flash_attention_fp8_even(q_fp8, k_fp8, v_fp8_padded, sm_scale_fp8)\n",
    "o_fp8_h = flash_attention_fp8_even(q_hadamard, k_hadamard, v_fp8_padded, sm_scale_fp8)\n",
    "o_half = torch.nn.functional.scaled_dot_product_attention(q, k, v, scale=sm_scale)\n",
    "o_half_ref = torch.nn.functional.scaled_dot_product_attention(q.to(torch.float8_e4m3fn).half(), k.to(torch.float8_e4m3fn).half(), v.to(torch.float8_e4m3fn).half(), scale=sm_scale).to(torch.float8_e4m3fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def diff_max(a, b):\n",
    "    return (a.float() - b.float()).abs().max()\n",
    "\n",
    "def diff_quantiles(a, b):\n",
    "    return torch.quantile((a.float() - b.float()).abs()[1, :, :], torch.tensor([0.25, 0.5, 0.75, 0.9, 0.99, 1.0]).cuda())\n",
    "\n",
    "def diff_rms(a, b):\n",
    "    return torch.sqrt(((a.float() - b.float()).square().sum()/a.numel()))\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    a = a.float()\n",
    "    b = b.float()\n",
    "    a_len = a.norm(dim=-1, p=2)\n",
    "    b_len = b.norm(dim=-1, p=2)\n",
    "    dot_prod = (a * b).sum(dim=-1)\n",
    "    return dot_prod/(a_len*b_len)\n",
    "\n",
    "\n",
    "diff_fp8 = diff_max(o_fp8, o_half)\n",
    "diff_h = diff_max(o_fp8_h, o_half)\n",
    "diff_ideal = diff_max(o_half, o_half.to(torch.float8_e4m3fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0153, device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_rms(o_half, o_fp8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0139, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_rms(o_half, o_fp8_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0134, device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_rms(o_half, o_half_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0078, 0.0312, 0.0625, 0.8125], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_quantiles(o_half_ref, o_fp8_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0156, 0.0625, 0.5000], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_quantiles(o_half_ref, o_fp8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0018, 0.0052, 0.0146, 0.0303, 0.0898, 0.6367], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_quantiles(o_half, o_half_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0021, 0.0060, 0.0173, 0.0364, 0.1309, 1.1367], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_quantiles(o_half, o_fp8_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0020, 0.0056, 0.0156, 0.0320, 0.1147, 0.7324], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_quantiles(o_half, o_fp8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0018, 0.0052, 0.0146, 0.0303, 0.0898, 0.6367], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_quantiles(o_half, o_half_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9684, device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(o_half, o_half_ref).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9909, 0.9973, 0.9987, 0.9996, 0.9997], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.quantile(cos_sim(o_fp8_h, o_half), torch.tensor([0.01, 0.25, 0.5, 0.75, 0.9]).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9933, 0.9979, 0.9989, 0.9996, 0.9997], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.quantile(cos_sim(o_fp8, o_half), torch.tensor([0.01, 0.25, 0.5, 0.75, 0.9]).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9681, device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(o_fp8, o_half).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6., device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_fp8.float().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.0664, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_half.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6., device='cuda:0')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_fp8_h.float().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0131, device='cuda:0')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((o_fp8_h.float() * o_half.float()) < 0).sum()/o_half.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0113, device='cuda:0')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((o_fp8.float() * o_half.float()) < 0).sum()/o_half.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0102, device='cuda:0')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((o_half.float() * o_half_ref.float()) < 0).sum()/o_half.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0023, device='cuda:0')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((o_fp8.float() * o_half_ref.float()) < 0).sum()/o_half.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0058, device='cuda:0')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((o_fp8_h.float() * o_half_ref.float()) < 0).sum()/o_half.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
